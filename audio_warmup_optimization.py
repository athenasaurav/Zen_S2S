#!/usr/bin/env python3
"""
Multi-GPU VITA-Audio Implementation (Based on Working Code)
- GPU 0: Text generation (VITA-Audio model)
- GPU 1: Audio decoding (GLM4Voice tokenizer)

Expected Performance:
- Audio chunk time: 922ms ‚Üí ~200ms (4-5x faster)
- Parallel text and audio processing

NEW FEATURE: First Complete Sentence Detection
- Detects and logs when the first complete sentence is generated
- Uses punctuation markers: . ! ? : , ; etc.
- Tracks timing from generation start to first complete sentence
"""

import math
import os
import sys
import time
import warnings
import re
from datetime import datetime, timezone
from threading import Thread
from queue import Queue
import threading
import queue

import gradio as gr
import numpy as np
import torch
from numba import jit
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

# Add GLM-4-Voice paths (ENABLE GLM-4-Voice tokenizer)
if True:
    # glm4voice tokenizer
    sys.path.append("third_party/GLM-4-Voice/")
    sys.path.append("third_party/GLM-4-Voice/cosyvoice/")
    sys.path.append("third_party/GLM-4-Voice/third_party/Matcha-TTS/")

    # Updated paths to match your setup
    audio_tokenizer_path = "./models/THUDM/glm-4-voice-tokenizer"
    flow_path = "./models/THUDM/glm-4-voice-decoder"
    audio_tokenizer_type = "glm4voice"
    model_name_or_path = "./models/VITA-MLLM/VITA-Audio-Boost"

# Import VITA-Audio modules
try:
    from vita_audio.data.processor.audio_processor import add_audio_input_contiguous
    from vita_audio.tokenizer import get_audio_tokenizer
    AUDIO_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è  Audio modules not available: {e}")
    AUDIO_MODULES_AVAILABLE = False

PUNCTUATION = "!?.,;:~‚Ä¶@#$%^&*()_+-=[]{}|\\`\"'<>/\n\t "

# Sentence-ending punctuation for first sentence detection
SENTENCE_ENDINGS = ".!?:;‚Ä¶"

def get_utc_timestamp():
    """Get current UTC timestamp in ISO format"""
    return datetime.now(timezone.utc).isoformat()

def log_event(event_type, task_type, message, **kwargs):
    """Log event with UTC timestamp and task context"""
    timestamp = get_utc_timestamp()
    task_info = f"[{task_type}]" if task_type else ""
    print(f"{timestamp} {task_info} {event_type}: {message}")

def detect_first_sentence_completion(accumulated_text, clean_text_only=True):
    """
    Detect if the accumulated text contains a complete sentence.
    
    Args:
        accumulated_text (str): The accumulated text so far
        clean_text_only (bool): If True, only consider clean text (no audio tokens)
    
    Returns:
        tuple: (is_complete, sentence_text, remaining_text)
    """
    text_to_check = accumulated_text
    
    if clean_text_only:
        # Remove audio tokens and system artifacts for sentence detection
        text_to_check = re.sub(r'<\|audio_\d+\|>', '', text_to_check)
        text_to_check = re.sub(r'<\|begin_of_audio\|>.*?<\|end_of_audio\|>', '', text_to_check, flags=re.DOTALL)
        text_to_check = text_to_check.replace('<|audio|>', '')
        
        # Remove system artifacts
        system_artifacts = [
            "You are a helpful AI assistant.",
            "You are a helpful AI assistant",
            "Convert the speech to text.",
            "Convert the text to speech.",
            "<|im_start|>", "<|im_end|>",
            "system", "user", "assistant"
        ]
        
        for artifact in system_artifacts:
            text_to_check = text_to_check.replace(artifact, "")
        
        # Clean up whitespace
        text_to_check = re.sub(r'\s+', ' ', text_to_check).strip()
    
    # Skip if text is too short or empty
    if len(text_to_check.strip()) < 3:
        return False, "", text_to_check
    
    # Look for sentence endings
    for i, char in enumerate(text_to_check):
        if char in SENTENCE_ENDINGS:
            # Found a sentence ending
            sentence = text_to_check[:i+1].strip()
            remaining = text_to_check[i+1:].strip()
            
            # Validate it's a meaningful sentence (not just punctuation)
            sentence_words = re.findall(r'\b\w+\b', sentence)
            if len(sentence_words) >= 2:  # At least 2 words for a meaningful sentence
                return True, sentence, remaining
    
    return False, "", text_to_check

@jit
def wav_to_int16(audio: np.ndarray) -> np.ndarray:
    am = int(math.ceil(float(np.abs(audio).max())) * 32768)
    am = 32767 * 32768 // am
    return np.multiply(audio, am).astype(np.int16)

def is_wav(file_path):
    if file_path is None:
        return False
    wav_extensions = [".wav", ".mp3", ".flac", ".ogg"]
    _, ext = os.path.splitext(file_path)
    return ext.lower() in wav_extensions

def find_warmup_audio_files():
    """Find available audio files in assets folder for warmup"""
    asset_dir = "asset"
    warmup_files = []
    
    if os.path.exists(asset_dir):
        for file in os.listdir(asset_dir):
            if file.endswith(('.wav', '.mp3')):
                file_path = os.path.join(asset_dir, file)
                if os.path.getsize(file_path) > 1000:  # Skip very small files
                    warmup_files.append(file_path)
    
    return warmup_files[:3]  # Use first 3 files for warmup

def parse_text(text):
    lines = text.split("\n")
    lines = [line for line in lines if line != ""]
    count = 0
    for i, line in enumerate(lines):
        if "```" in line:
            count += 1
            items = line.split("`")
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = "<br></code></pre>"
        else:
            if i > 0 and count % 2 == 1:
                line = line.replace("`", "\\`")
                line = line.replace("<", "&lt;")
                line = line.replace(">", "&gt;")
                line = line.replace(" ", "&nbsp;")
                line = line.replace("*", "&#42;")
                line = line.replace("_", "&#95;")
                line = line.replace("-", "&#45;")
                line = line.replace(".", "&#46;")
                line = line.replace("!", "&#33;")
                line = line.replace("(", "&#40;")
                line = line.replace(")", "&#41;")
                line = line.replace("$", "&#36;")
            lines[i] = "<br>" + line
    return "".join(lines)

def find_audio_segments_regex(text):
    """Find all substrings between <|begin_of_audio|> and <|end_of_audio|> using regex."""
    pattern = re.compile(r"<\|begin_of_audio\|>(.*?)<\|end_of_audio\|>", re.DOTALL)
    segments = pattern.findall(text)
    return [segment.strip() for segment in segments]

def extract_token_ids_as_int(text):
    """Extract token IDs from audio segments and return as integers."""
    pattern = re.compile(r"<\|audio_(\d+)\|>")
    token_ids = pattern.findall(text)
    return [int(id) for id in token_ids if id.strip()]

def extract_assistant_audio_tokens_only(output_text, audio_offset):
    """Extract ONLY assistant's audio tokens, excluding user's audio tokens"""
    log_event("EXTRACT", "AUDIO_TOKENS", "Extracting ONLY assistant audio tokens...")
    
    # Find the assistant section
    assistant_marker = "<|im_start|>assistant"
    assistant_start = output_text.find(assistant_marker)
    
    if assistant_start == -1:
        log_event("EXTRACT", "AUDIO_TOKENS", "No assistant section found!")
        return []
    
    # Extract only the assistant's part
    assistant_section = output_text[assistant_start:]
    
    # Find all audio segments in assistant section only
    assistant_audio_segments = find_audio_segments_regex(assistant_section)
    
    # Extract token IDs from assistant's audio segments only
    assistant_audio_tokens = []
    for segment in assistant_audio_segments:
        tokens = extract_token_ids_as_int(segment)
        assistant_audio_tokens.extend(tokens)
    
    log_event("EXTRACT", "AUDIO_TOKENS", f"Total assistant audio tokens: {len(assistant_audio_tokens)}")
    return assistant_audio_tokens

def clean_text_display(text, task_type="Spoken QA"):
    """Enhanced text cleaning to remove system message artifacts and audio tokens"""
    
    log_event("CLEAN", task_type, f"Cleaning text for {task_type}")
    
    # Remove system/user/assistant markers
    clean_text = text
    clean_text = clean_text.replace("<|im_start|>", "").replace("<|im_end|>", "")
    clean_text = re.sub(r"(system|user|assistant)\s*", "", clean_text)
    
    # Extract audio segments first (for counting)
    audio_segments = find_audio_segments_regex(clean_text)
    total_audio_tokens = sum(len(extract_token_ids_as_int(segment)) for segment in audio_segments)
    
    # Remove ALL audio-related tokens for clean display
    clean_text = re.sub(r"<\|begin_of_audio\|>.*?<\|end_of_audio\|>", "", clean_text, flags=re.DOTALL)
    clean_text = re.sub(r"<\|audio_\d+\|>", "", clean_text)
    clean_text = clean_text.replace("<|audio|>", "")
    
    # Remove system artifacts
    system_artifacts = [
        "You are a helpful AI assistant.",
        "You are a helpful AI assistant",
        "Convert the speech to text.",
        "Convert the text to speech.",
    ]
    
    for artifact in system_artifacts:
        clean_text = clean_text.replace(artifact, "")
    
    # Clean up whitespace
    clean_text = re.sub(r"\n\s*\n", "\n", clean_text)
    clean_text = re.sub(r"^\s+|\s+$", "", clean_text)
    clean_text = re.sub(r"\s+", " ", clean_text)
    clean_text = re.sub(r"^[.\s,;:!?]+", "", clean_text)
    
    final_text = clean_text.strip()
    
    log_event("CLEAN", task_type, f"Final cleaned text: '{final_text}'")
    
    return final_text, len(audio_segments), total_audio_tokens

class MultiGPUAudioProcessor:
    """Audio processor that runs on dedicated GPU with background worker"""
    
    def __init__(self, audio_tokenizer, device="cuda:1", sample_rate=16000):
        self.audio_tokenizer = audio_tokenizer
        self.device = device
        self.sample_rate = sample_rate
        self.audio_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.is_running = False
        self.worker_thread = None
        self.is_warmed_up = False
        
        log_event("MULTI_GPU", "AUDIO", f"Audio processor initialized on {device}")
    
    def start_worker(self):
        """Start background audio processing worker"""
        if self.is_running:
            return
            
        self.is_running = True
        self.worker_thread = threading.Thread(target=self._audio_worker, daemon=True)
        self.worker_thread.start()
        log_event("MULTI_GPU", "AUDIO", "Audio worker thread started")
    
    def stop_worker(self):
        """Stop background audio processing worker"""
        self.is_running = False
        if self.worker_thread:
            self.worker_thread.join(timeout=5.0)
        log_event("MULTI_GPU", "AUDIO", "Audio worker thread stopped")
    
    def _audio_worker(self):
        """Background worker for audio processing on dedicated GPU"""
        torch.cuda.set_device(self.device)
        
        while self.is_running:
            try:
                # Get audio processing job
                job = self.audio_queue.get(timeout=1.0)
                if job is None:  # Shutdown signal
                    break
                
                chunk_id, audio_tokens, start_time = job
                
                # Process audio on dedicated GPU
                with torch.cuda.device(self.device):
                    chunk_start_time = time.time()
                    
                    # Decode audio tokens
                    audio_chunk = self.audio_tokenizer.decode(audio_tokens)
                    
                    chunk_time = time.time() - chunk_start_time
                    total_time = time.time() - start_time
                
                # Return result
                result = {
                    'chunk_id': chunk_id,
                    'audio_chunk': audio_chunk,
                    'chunk_time': chunk_time,
                    'total_time': total_time,
                    'tokens': len(audio_tokens),
                    'audio_duration': audio_chunk.shape[0]/self.sample_rate if hasattr(audio_chunk, 'shape') else 0
                }
                
                self.result_queue.put(result)
                
            except queue.Empty:
                continue
            except Exception as e:
                log_event("MULTI_GPU", "AUDIO", f"Audio worker error: {e}")
                continue
    
    def warmup_audio_decoder(self):
        """Pre-warm the audio decoder using assets folder files"""
        if self.is_warmed_up:
            log_event("MULTI_GPU", "WARMUP", "Audio decoder already warmed up")
            return
            
        log_event("MULTI_GPU", "WARMUP", "üî• Starting multi-GPU audio decoder warmup...")
        warmup_start_time = time.time()
        
        # Set device for warmup
        torch.cuda.set_device(self.device)
        
        # Find warmup audio files
        warmup_files = find_warmup_audio_files()
        
        if not warmup_files:
            log_event("MULTI_GPU", "WARMUP", "‚ö†Ô∏è No warmup files found, creating dummy tokens")
            # Create dummy audio tokens for warmup
            dummy_tokens = [1000, 2000, 3000, 4000]  # 4 dummy tokens
            try:
                with torch.cuda.device(self.device):
                    warmup_chunk_start = time.time()
                    dummy_audio = self.audio_tokenizer.decode(dummy_tokens)
                    warmup_chunk_time = time.time() - warmup_chunk_start
                    log_event("MULTI_GPU", "WARMUP", f"Dummy warmup chunk: {len(dummy_tokens)} tokens ‚Üí {dummy_audio.shape[0]/self.sample_rate:.2f}s audio in {warmup_chunk_time:.3f}s")
            except Exception as e:
                log_event("MULTI_GPU", "WARMUP", f"Dummy warmup failed: {e}")
        else:
            # Use real audio files for warmup
            for i, warmup_file in enumerate(warmup_files):
                try:
                    log_event("MULTI_GPU", "WARMUP", f"Warming up with: {os.path.basename(warmup_file)}")
                    
                    with torch.cuda.device(self.device):
                        # Encode audio to tokens
                        encode_start = time.time()
                        warmup_tokens = self.audio_tokenizer.encode(warmup_file)
                        encode_time = time.time() - encode_start
                        
                        # Take first 4 tokens for warmup chunk
                        warmup_chunk_tokens = warmup_tokens[:4] if len(warmup_tokens) >= 4 else warmup_tokens
                        
                        # Decode tokens to audio (this warms up the decoder)
                        decode_start = time.time()
                        warmup_audio = self.audio_tokenizer.decode(warmup_chunk_tokens)
                        decode_time = time.time() - decode_start
                        
                        log_event("MULTI_GPU", "WARMUP", f"Warmup {i+1}: encode={encode_time:.3f}s, decode={decode_time:.3f}s, tokens={len(warmup_chunk_tokens)}")
                        
                        # Only need one successful warmup
                        if decode_time < 0.8:  # If decode is fast, we're warmed up
                            break
                            
                except Exception as e:
                    log_event("MULTI_GPU", "WARMUP", f"Warmup file {warmup_file} failed: {e}")
                    continue
        
        warmup_total_time = time.time() - warmup_start_time
        self.is_warmed_up = True
        log_event("MULTI_GPU", "WARMUP", f"üî• Multi-GPU audio decoder warmup completed in {warmup_total_time:.3f}s")
    
    def process_audio_chunk_async(self, chunk_id: int, audio_tokens: list, start_time: float):
        """Submit audio chunk for async processing"""
        job = (chunk_id, audio_tokens, start_time)
        self.audio_queue.put(job)
    
    def get_completed_chunk(self, timeout=0.1):
        """Get completed audio chunk if available"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None

class S2SInferenceMultiGPU:
    """Multi-GPU Speech-to-Speech Inference based on working code"""
    
    def __init__(self, model_name_or_path, audio_tokenizer_path, audio_tokenizer_type, flow_path, 
                 audio_tokenizer_rank=0, use_turbo=True, enable_warmup=True,
                 text_gpu="cuda:0", audio_gpu="cuda:1"):
        
        self.model_name_or_path = model_name_or_path
        self.audio_tokenizer_path = audio_tokenizer_path
        self.audio_tokenizer_type = audio_tokenizer_type
        self.flow_path = flow_path
        self.audio_tokenizer_rank = audio_tokenizer_rank
        self.use_turbo = use_turbo
        self.enable_warmup = enable_warmup
        self.text_gpu = text_gpu
        self.audio_gpu = audio_gpu
        
        # Load text model on primary GPU
        self._load_text_model()
        
        # Load audio components on dedicated GPU
        self._load_audio_components()
        
        # Setup multi-GPU processing
        self._setup_multi_gpu_processing()
        
        log_event("MULTI_GPU", "INIT", f"Multi-GPU VITA-Audio initialized")
        log_event("MULTI_GPU", "INIT", f"Text model on {text_gpu}, Audio processing on {audio_gpu}")
    
    def _load_text_model(self):
        """Load text generation model on primary GPU"""
        log_event("MULTI_GPU", "INIT", f"Loading text model on {self.text_gpu}...")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name_or_path,
            trust_remote_code=True,
        )
        log_event("MULTI_GPU", "INIT", "Tokenizer loaded")
        
        # Load model on specific GPU
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name_or_path,
            trust_remote_code=True,
            device_map={
                "": self.text_gpu  # Force all layers to text GPU
            },
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
        ).eval()
        log_event("MULTI_GPU", "INIT", f"Text model loaded on {self.text_gpu}")
        
        # Configure generation mode
        self.configure_generation_mode()
        
        # Get audio offset
        self.audio_offset = self.tokenizer.convert_tokens_to_ids("<|audio_0|>")
        log_event("MULTI_GPU", "INIT", f"Audio offset: {self.audio_offset}")
    
    def _load_audio_components(self):
        """Load audio components on dedicated GPU"""
        if not AUDIO_MODULES_AVAILABLE:
            log_event("MULTI_GPU", "INIT", "‚ö†Ô∏è Audio modules not available, skipping audio component loading")
            self.audio_tokenizer = None
            return
            
        log_event("MULTI_GPU", "INIT", f"Loading audio components on {self.audio_gpu}...")
        
        # Set device for audio loading
        torch.cuda.set_device(self.audio_gpu)
        
        try:
            # Load audio tokenizer on dedicated GPU
            self.audio_tokenizer = get_audio_tokenizer(
                self.audio_tokenizer_path,
                self.audio_tokenizer_type,
                flow_path=self.flow_path,
                rank=self.audio_tokenizer_rank,
            )
            log_event("MULTI_GPU", "INIT", f"Audio tokenizer loaded on {self.audio_gpu}")
            
        except Exception as e:
            log_event("MULTI_GPU", "INIT", f"Failed to load audio components: {e}")
            self.audio_tokenizer = None
    
    def _setup_multi_gpu_processing(self):
        """Setup multi-GPU processing components"""
        if self.audio_tokenizer:
            # Initialize streaming audio processor
            self.streaming_processor = MultiGPUAudioProcessor(
                self.audio_tokenizer, 
                device=self.audio_gpu
            )
            
            # Start background worker
            self.streaming_processor.start_worker()
            
            # Perform warmup if enabled
            if self.enable_warmup:
                self.streaming_processor.warmup_audio_decoder()
                
            log_event("MULTI_GPU", "INIT", "Multi-GPU streaming processor initialized")
        else:
            self.streaming_processor = None
            log_event("MULTI_GPU", "INIT", "No audio tokenizer available, streaming processor disabled")
    
    def configure_generation_mode(self):
        """Configure generation parameters based on mode"""
        if self.use_turbo:
            # Turbo mode: faster but potentially lower quality
            self.model.generation_config.mtp_inference_mode = [1, 10]
            log_event("MULTI_GPU", "CONFIG", "üöÄ Turbo mode enabled: MTP [1,10]")
        else:
            # Boost mode: balanced performance and quality
            self.model.generation_config.mtp_inference_mode = [1, 10, 4, 10]
            log_event("MULTI_GPU", "CONFIG", "‚ö° Boost mode enabled: MTP [1,10,4,10]")
        
        # Common generation settings
        self.model.generation_config.max_new_tokens = 8192
        self.model.generation_config.chat_format = "chatml"
        self.model.generation_config.use_cache = True
        self.model.generation_config.do_sample = False
        self.model.generation_config.temperature = 1.0
        self.model.generation_config.top_k = 50
        self.model.generation_config.top_p = 1.0
        self.model.generation_config.num_beams = 1
    
    def switch_mode(self, use_turbo: bool):
        """Switch between Turbo and Boost modes"""
        if self.use_turbo != use_turbo:
            self.use_turbo = use_turbo
            self.configure_generation_mode()
            mode_name = "TURBO" if use_turbo else "BOOST"
            log_event("MULTI_GPU", "CONFIG", f"Switched to {mode_name} mode")
    
    def run_infer_streaming(self, audio_path=None, prompt_audio_path=None, message="", 
                          task_type="Spoken QA", max_returned_tokens=4096, stream_stride=4):
        """
        Run streaming inference with multi-GPU optimization and first sentence detection
        
        NEW FEATURE: Detects and logs when the first complete sentence is generated
        """
        request_start_time = time.time()
        log_event("MULTI_GPU", task_type, f"üöÄ Starting multi-GPU streaming inference...")
        log_event("MULTI_GPU", task_type, f"Mode: {'TURBO' if self.use_turbo else 'BOOST'}")
        
        # Initialize streaming variables
        if self.streaming_processor:
            self.pending_chunks = {}
            self.completed_chunks = {}
            self.chunk_counter = 0
        
        # Prepare messages (same as working code)
        if task_type == "TTS":
            messages = [{"role": "system", "content": "You are a helpful AI assistant."}] + [
                {"role": "user", "content": f"Convert the text to speech.\n{message}"}
            ]
        elif task_type == "ASR":
            if audio_path is None:
                raise ValueError("ASR task requires audio_path")
            messages = [{"role": "system", "content": "You are a helpful AI assistant."}] + [
                {"role": "user", "content": "Convert the speech to text.\n<|audio|>"}
            ]
        else:  # Spoken QA
            if audio_path:
                messages = [{"role": "system", "content": "You are a helpful AI assistant."}] + [
                    {"role": "user", "content": "<|audio|>"}
                ]
            else:
                messages = [{"role": "system", "content": "You are a helpful AI assistant."}] + [
                    {"role": "user", "content": message}
                ]

        # Apply chat template
        template_start_time = time.time()
        input_ids = self.tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
        )
        template_time = time.time() - template_start_time
        log_event("MULTI_GPU", task_type, f"Chat template applied in {template_time:.3f}s")

        # Handle audio input processing (EXACT same as working code)
        audios = None
        audio_indices = None
        
        if (audio_path is not None or prompt_audio_path is not None) and self.audio_tokenizer:
            log_event("MULTI_GPU", task_type, f"Processing audio input with tokenizer...")
            
            # Check if audio tokenizer applies to user role
            if self.audio_tokenizer.apply_to_role("user", is_contiguous=True):
                log_event("MULTI_GPU", task_type, "Using contiguous codec for audio processing")
                # Contiguous codec
                audio_paths = []
                if audio_path is not None:
                    audio_paths.append(audio_path)
                if prompt_audio_path is not None:
                    audio_paths.append(prompt_audio_path)
                    
                try:
                    audio_process_start = time.time()
                    # EXACT same signature as working code
                    input_ids, audios, audio_indices = add_audio_input_contiguous(
                        input_ids, audio_paths, self.tokenizer, self.audio_tokenizer
                    )
                    audio_process_time = time.time() - audio_process_start
                    log_event("MULTI_GPU", task_type, f"Processed {len(audio_paths)} audio files in {audio_process_time:.3f}s")
                    
                except Exception as e:
                    log_event("MULTI_GPU", task_type, f"Audio processing error: {e}")
                    import traceback
                    traceback.print_exc()
                    audios = None
                    audio_indices = None
                    
            elif self.audio_tokenizer.apply_to_role("user", is_discrete=True):
                log_event("MULTI_GPU", task_type, "Using discrete codec for audio processing")
                # Discrete codec (same as working code)
                if audio_path is not None:
                    try:
                        audio_encode_start = time.time()
                        audio_tokens = self.audio_tokenizer.encode(audio_path)
                        audio_encode_time = time.time() - audio_encode_start
                        audio_tokens_str = "".join([f"<|audio_{i}|>" for i in audio_tokens])
                        log_event("MULTI_GPU", task_type, f"Encoded audio to {len(audio_tokens)} tokens in {audio_encode_time:.3f}s")
                        
                        # Replace <|audio|> in the input with actual audio tokens
                        input_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=False)
                        input_text = input_text.replace("<|audio|>", f"<|begin_of_audio|>{audio_tokens_str}<|end_of_audio|>")
                        
                        # Re-tokenize with audio tokens
                        input_ids = self.tokenizer.encode(input_text, return_tensors="pt")
                        
                    except Exception as e:
                        log_event("MULTI_GPU", task_type, f"Discrete audio processing error: {e}")

        # Move to text GPU
        input_ids = input_ids.to(self.text_gpu)

        # Setup streaming generation
        streamer = TextIteratorStreamer(
            self.tokenizer,
            timeout=60.0,
            skip_prompt=True,
            skip_special_tokens=False
        )
        
        generation_kwargs = {
            "input_ids": input_ids,
            "streamer": streamer,
            "max_new_tokens": max_returned_tokens,
            "use_cache": True,
            "num_logits_to_keep": 1,
        }
        
        if audios is not None:
            generation_kwargs["audios"] = audios.to(self.text_gpu)
        if audio_indices is not None:
            generation_kwargs["audio_indices"] = audio_indices

        # Start generation in separate thread
        generation_start_time = time.time()
        generation_thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        generation_thread.start()
        
        # Process streaming tokens with multi-GPU audio processing
        generated_text = ""
        current_audio_tokens = []
        
        # Timing tracking variables
        first_token_time = None
        first_text_token_time = None
        first_audio_token_time = None
        first_audio_chunk_time = None
        
        # NEW: First sentence detection variables
        first_sentence_time = None
        first_sentence_text = ""
        accumulated_text_for_sentence = ""
        sentence_detected = False
        
        log_event("MULTI_GPU", task_type, "üéØ Starting real-time token processing...")
        log_event("SENTENCE_DETECT", task_type, "üìù Starting first sentence detection...")
        
        for new_text in streamer:
            current_time = time.time()
            token_latency = current_time - generation_start_time
            
            # Track first token
            if first_token_time is None:
                first_token_time = token_latency
                log_event("MULTI_GPU", task_type, f"üéØ FIRST TOKEN at {first_token_time:.3f}s")
            
            # Track first text token
            if new_text.strip() and first_text_token_time is None:
                first_text_token_time = token_latency
                log_event("MULTI_GPU", task_type, f"üìù FIRST TEXT TOKEN at {first_text_token_time:.3f}s: '{new_text.strip()}'")
            
            # NEW: Accumulate text for sentence detection
            accumulated_text_for_sentence += new_text
            
            # NEW: Check for first complete sentence
            if not sentence_detected:
                is_complete, sentence_text, remaining_text = detect_first_sentence_completion(
                    accumulated_text_for_sentence, clean_text_only=True
                )
                
                if is_complete and sentence_text:
                    first_sentence_time = token_latency
                    first_sentence_text = sentence_text
                    sentence_detected = True
                    
                    log_event("SENTENCE_DETECT", task_type, f"üéâ FIRST COMPLETE SENTENCE at {first_sentence_time:.3f}s")
                    log_event("SENTENCE_DETECT", task_type, f"üìù Sentence: '{first_sentence_text}'")
                    log_event("SENTENCE_DETECT", task_type, f"üìä Sentence length: {len(first_sentence_text)} characters")
                    log_event("SENTENCE_DETECT", task_type, f"üìä Word count: {len(first_sentence_text.split())}")
            
            # Extract audio tokens
            audio_tokens_in_text = re.findall(r'<\|audio_(\d+)\|>', new_text)
            
            for token_str in audio_tokens_in_text:
                if token_str.strip():
                    try:
                        audio_token_id = int(token_str)
                        current_audio_tokens.append(audio_token_id)
                        
                        # Track first audio token
                        if first_audio_token_time is None:
                            first_audio_token_time = token_latency
                            log_event("MULTI_GPU", task_type, f"üéµ FIRST AUDIO TOKEN at {first_audio_token_time:.3f}s")
                        
                    except ValueError:
                        continue
            
            # Process audio chunks when we have enough tokens (multi-GPU)
            if len(current_audio_tokens) >= stream_stride and self.streaming_processor:
                chunk_tokens = current_audio_tokens[:stream_stride]
                current_audio_tokens = current_audio_tokens[stream_stride:]
                
                # Submit for async audio processing on GPU 1
                self.streaming_processor.process_audio_chunk_async(
                    self.chunk_counter, 
                    chunk_tokens, 
                    request_start_time
                )
                
                self.pending_chunks[self.chunk_counter] = {
                    'tokens': chunk_tokens,
                    'submit_time': current_time
                }
                
                self.chunk_counter += 1
            
            # Check for completed audio chunks
            self._process_completed_chunks(task_type, request_start_time)
            
            generated_text += new_text
        
        # Process remaining audio tokens
        if current_audio_tokens and self.streaming_processor:
            self.streaming_processor.process_audio_chunk_async(
                self.chunk_counter, 
                current_audio_tokens, 
                request_start_time
            )
            
            self.pending_chunks[self.chunk_counter] = {
                'tokens': current_audio_tokens,
                'submit_time': time.time()
            }
            
            self.chunk_counter += 1
        
        # Wait for all audio chunks to complete
        self._wait_for_all_chunks(task_type)
        
        # Generation complete
        generation_thread.join(timeout=10.0)
        
        total_time = time.time() - request_start_time
        generation_time = time.time() - generation_start_time
        
        # Log performance summary
        log_event("MULTI_GPU", task_type, "=== MULTI-GPU TIMING RESULTS ===")
        log_event("MULTI_GPU", task_type, f"Text GPU: {self.text_gpu}")
        log_event("MULTI_GPU", task_type, f"Audio GPU: {self.audio_gpu}")
        log_event("MULTI_GPU", task_type, f"üéØ First token latency: {first_token_time:.3f}s")
        if first_text_token_time:
            log_event("MULTI_GPU", task_type, f"üìù First text token latency: {first_text_token_time:.3f}s")
        if first_audio_token_time:
            log_event("MULTI_GPU", task_type, f"üéµ First audio token latency: {first_audio_token_time:.3f}s")
        if first_audio_chunk_time:
            log_event("MULTI_GPU", task_type, f"üéµ First audio chunk latency: {first_audio_chunk_time:.3f}s ‚ö° (multi-GPU optimized!)")
        
        # NEW: Log first sentence detection results
        if first_sentence_time:
            log_event("SENTENCE_DETECT", task_type, "=== FIRST SENTENCE DETECTION RESULTS ===")
            log_event("SENTENCE_DETECT", task_type, f"üéâ First sentence latency: {first_sentence_time:.3f}s")
            log_event("SENTENCE_DETECT", task_type, f"üìù First sentence: '{first_sentence_text}'")
            log_event("SENTENCE_DETECT", task_type, f"üìä Sentence characteristics:")
            log_event("SENTENCE_DETECT", task_type, f"   - Length: {len(first_sentence_text)} characters")
            log_event("SENTENCE_DETECT", task_type, f"   - Words: {len(first_sentence_text.split())}")
            log_event("SENTENCE_DETECT", task_type, f"   - Ending: '{first_sentence_text[-1] if first_sentence_text else 'N/A'}'")
            log_event("SENTENCE_DETECT", task_type, "=== END SENTENCE DETECTION ===")
        else:
            log_event("SENTENCE_DETECT", task_type, "‚ö†Ô∏è No complete sentence detected during generation")
        
        log_event("MULTI_GPU", task_type, f"üéµ Audio chunks generated: {len(self.completed_chunks)}")
        log_event("MULTI_GPU", task_type, f"Total request time: {total_time:.3f}s")
        log_event("MULTI_GPU", task_type, f"Generation time: {generation_time:.3f}s")
        log_event("MULTI_GPU", task_type, "=== END MULTI-GPU TIMING ===")
        
        return clean_text_display(generated_text, task_type)[0]
    
    def _process_completed_chunks(self, task_type: str, request_start_time: float):
        """Process completed audio chunks from background worker"""
        while True:
            result = self.streaming_processor.get_completed_chunk()
            if result is None:
                break
            
            chunk_id = result['chunk_id']
            chunk_time = result['chunk_time']
            tokens = result['tokens']
            audio_duration = result['audio_duration']
            
            # Track first audio chunk time
            if chunk_id == 0:
                first_audio_chunk_time = time.time() - request_start_time
                log_event("MULTI_GPU", task_type, f"üéµ FIRST AUDIO CHUNK at {first_audio_chunk_time:.3f}s ‚ö° (multi-GPU optimized!)")
            
            # Log chunk completion
            performance_note = " ‚ö° (multi-GPU optimized!)"
            log_event("MULTI_GPU", "STREAM", 
                     f"Chunk {chunk_id}: {tokens} tokens ‚Üí {audio_duration:.2f}s audio in {chunk_time:.3f}s{performance_note}")
            
            # Store completed chunk
            self.completed_chunks[chunk_id] = result
            
            # Remove from pending
            if chunk_id in self.pending_chunks:
                del self.pending_chunks[chunk_id]
    
    def _wait_for_all_chunks(self, task_type: str, timeout: float = 30.0):
        """Wait for all pending audio chunks to complete"""
        wait_start = time.time()
        
        while self.pending_chunks and (time.time() - wait_start) < timeout:
            self._process_completed_chunks(task_type, wait_start)
            time.sleep(0.01)  # Small delay
        
        if self.pending_chunks:
            log_event("MULTI_GPU", task_type, f"Warning: {len(self.pending_chunks)} chunks still pending after timeout")
    
    def __del__(self):
        """Cleanup multi-GPU resources"""
        if hasattr(self, 'streaming_processor') and self.streaming_processor:
            self.streaming_processor.stop_worker()

def create_gradio_interface():
    """Create Gradio interface for multi-GPU VITA-Audio"""
    
    # Initialize multi-GPU engine
    log_event("MULTI_GPU", "MAIN", "üî• Initializing Multi-GPU VITA-Audio Engine...")
    
    s2s_engine = S2SInferenceMultiGPU(
        model_name_or_path=model_name_or_path,
        audio_tokenizer_path=audio_tokenizer_path,
        audio_tokenizer_type=audio_tokenizer_type,
        flow_path=flow_path,
        use_turbo=True,  # Start with Turbo mode
        enable_warmup=True,  # Enable warmup
        text_gpu="cuda:0",
        audio_gpu="cuda:1"
    )
    
    log_event("MULTI_GPU", "MAIN", "‚úÖ Multi-GPU Engine initialized successfully!")
    
    def chat_interface(audio_input, task_selector, use_turbo_mode, history):
        """Main chat interface function"""
        if audio_input is None or audio_input == "":
            return history, None
        
        # Switch mode if needed
        s2s_engine.switch_mode(use_turbo_mode)
        
        mode_name = "TURBO" if use_turbo_mode else "BOOST"
        log_event("MULTI_GPU", task_selector, f"Using {mode_name} mode (üî• Multi-GPU)")
        
        try:
            # Run multi-GPU inference
            response = s2s_engine.run_infer_streaming(
                audio_path=audio_input,
                task_type=task_selector
            )
            
            # Update chat history
            history.append([f"[Audio Input] {os.path.basename(audio_input)}", response])
            
            return history, None
            
        except Exception as e:
            error_msg = f"Multi-GPU processing error: {e}"
            log_event("MULTI_GPU", task_selector, error_msg)
            history.append([f"[Audio Input] {os.path.basename(audio_input)}", error_msg])
            return history, None
    
    # Create Gradio interface
    with gr.Blocks(title="Multi-GPU VITA-Audio", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# üöÄ Multi-GPU VITA-Audio (4x H100 Optimized)")
        gr.Markdown("**Text Generation: GPU:0 | Audio Processing: GPU:1 | Expected 4-5x faster audio chunks**")
        gr.Markdown("**NEW: üìù First Complete Sentence Detection & Logging**")
        
        with gr.Row():
            with gr.Column(scale=2):
                # Audio input
                audio_input = gr.Audio(
                    label="Audio Input", 
                    type="filepath",
                    format="wav"
                )
                
                # Task selector
                task_selector = gr.Dropdown(
                    choices=["Spoken QA", "ASR", "TTS"],
                    value="Spoken QA",
                    label="Task Type"
                )
                
                # Mode selector
                use_turbo_mode = gr.Checkbox(
                    label="üöÄ Turbo Mode ([1,10] vs [1,10,4,10])",
                    value=True
                )
                
                # Submit button
                submit_btn = gr.Button("Submit", variant="primary")
            
            with gr.Column(scale=3):
                # Chat history
                chatbot = gr.Chatbot(
                    label="Multi-GPU Chat History", 
                    height=500
                )
                
                # Clear button
                clear_btn = gr.Button("Clear History")
        
        # Performance info
        gr.Markdown("""
        ### üî• Multi-GPU Performance Optimizations:
        - **GPU 0 (cuda:0)**: Text generation (VITA-Audio model)
        - **GPU 1 (cuda:1)**: Audio decoding (GLM4Voice tokenizer)  
        - **Expected**: 4-5x faster audio chunk generation (~200ms vs 900ms)
        - **Warmup**: Pre-warmed audio decoder using assets folder
        - **Parallel processing**: Text and audio generation simultaneously
        
        ### üìù NEW: First Sentence Detection:
        - **Real-time detection**: Identifies complete sentences as they're generated
        - **Punctuation-based**: Uses . ! ? : ; etc. to detect sentence endings
        - **Timing logs**: Records exact time when first sentence is completed
        - **Detailed metrics**: Logs sentence length, word count, and characteristics
        """)
        
        # Event handlers
        submit_btn.click(
            chat_interface,
            inputs=[audio_input, task_selector, use_turbo_mode, chatbot],
            outputs=[chatbot, audio_input]
        )
        
        clear_btn.click(
            lambda: [],
            outputs=[chatbot]
        )
    
    return demo

if __name__ == "__main__":
    log_event("MULTI_GPU", "MAIN", "üöÄ Starting Multi-GPU VITA-Audio with First Sentence Detection...")
    
    # Create and launch interface
    demo = create_gradio_interface()
    
    # Launch with multi-GPU configuration
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        debug=True,
        show_error=True
    )
